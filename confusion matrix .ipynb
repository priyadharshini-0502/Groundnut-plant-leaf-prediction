{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b9652a-f4fd-4c70-8fac-0b366c106de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927ad015-b07e-4610-91ca-621596da5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_file)\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(image_path)\n",
    "                # Convert image to numpy array or do any preprocessing\n",
    "                # image = np.array(image)\n",
    "                \n",
    "                # Append image to the list\n",
    "                images.append(image)\n",
    "                \n",
    "                # Append label to the list\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {image_path}. Error: {e}\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f6a98f-e312-4369-8098-9602bae9ef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ramkumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Ramkumar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to load the trained model\n",
    "def load_trained_model(model_path):\n",
    "    model = load_model(model_path)  # Load the model from .h5 file\n",
    "    return model\n",
    "\n",
    "# Assuming you have a dataset directory named 'test_dataset' containing subdirectories for each class\n",
    "dataset_dir = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/pretrained_testf'\n",
    "\n",
    "# Load test data (features) and corresponding true labels\n",
    "test_images, y_true = load_data(dataset_dir)\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/DenseNet169.h5'  # Path to your saved model\n",
    "model = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004dff1c-1c36-42de-b34b-9c3176917b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 365s 4s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "def predict_labels(model, images):\n",
    "    # Preprocess images according to DenseNet requirements\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # Resize image to target input dimensions expected by DenseNet (e.g., 224x224 for DenseNet121)\n",
    "        img = img.resize((256, 256))  # Adjust size as per your DenseNet model\n",
    "        \n",
    "        # Convert image to array and apply preprocessing required by DenseNet\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        processed_images.append(img_array)\n",
    "\n",
    "    # Convert processed images to numpy array\n",
    "    processed_images = np.array(processed_images)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(processed_images)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Assuming you have a pre-trained DenseNet model stored in the variable 'model'\n",
    "# And 'test_images' contains the images you want to make predictions on\n",
    "# 'test_images' is assumed to be a list of PIL Image objects\n",
    "\n",
    "# Make predictions using your trained model\n",
    "y_pred = predict_labels(model, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc793022-b84c-4ee9-9f5b-531c4c19d3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa4cded-1780-4679-83a6-37dc46dc8153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_leaf_spot_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'early_rust_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " 'healthy_leaf_1',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bdf8ea9-a4eb-4d39-90bb-ba979ee103b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "\n",
      "Confusion Matrix:\n",
      "[[357   0  11  41   0   0]\n",
      " [  0 403   0   6   0   0]\n",
      " [ 86   0 295  27   1   0]\n",
      " [  7   0   0 398   0   0]\n",
      " [  0   0   5   1 404   0]\n",
      " [  0  27   0 114   0 268]]\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     early_leaf_spot_1       0.79      0.87      0.83       409\n",
      "          early_rust_1       0.94      0.99      0.96       409\n",
      "        healthy_leaf_1       0.95      0.72      0.82       409\n",
      "      late_leaf_spot_1       0.68      0.98      0.80       405\n",
      "nutrition_deficiency_1       1.00      0.99      0.99       410\n",
      "                rust_1       1.00      0.66      0.79       409\n",
      "\n",
      "              accuracy                           0.87      2451\n",
      "             macro avg       0.89      0.87      0.87      2451\n",
      "          weighted avg       0.89      0.87      0.87      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"early_leaf_spot_1\", \"early_rust_1\", \"healthy_leaf_1\", \"late_leaf_spot_1\", \"nutrition_deficiency_1\", \"rust_1\"]  # Replace with your actual class names\n",
    "\n",
    "# Map numerical predictions to class names\n",
    "y_pred_mapped = [class_names[prediction] for prediction in y_pred]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_mapped)\n",
    "\n",
    "# Create a confusion matrix using the actual class names\n",
    "cm = confusion_matrix(y_true, y_pred_mapped, labels=class_names)\n",
    "\n",
    "# Generate a classification report using the actual class names\n",
    "report = classification_report(y_true, y_pred_mapped, target_names=class_names)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87c102d0-ac76-4c46-86ea-839232e55e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n",
      "\n",
      "Confusion Matrix:\n",
      "[[357   0  11  41   0   0]\n",
      " [  0 403   0   6   0   0]\n",
      " [ 86   0 295  27   1   0]\n",
      " [  7   0   0 398   0   0]\n",
      " [  0   0   5   1 404   0]\n",
      " [  0  27   0 114   0 268]]\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     early_leaf_spot_1       0.79      0.87      0.83       409\n",
      "          early_rust_1       0.94      0.99      0.96       409\n",
      "        healthy_leaf_1       0.95      0.72      0.82       409\n",
      "      late_leaf_spot_1       0.68      0.98      0.80       405\n",
      "nutrition_deficiency_1       1.00      0.99      0.99       410\n",
      "                rust_1       1.00      0.66      0.79       409\n",
      "\n",
      "              accuracy                           0.87      2451\n",
      "             macro avg       0.89      0.87      0.87      2451\n",
      "          weighted avg       0.89      0.87      0.87      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"early_leaf_spot_1\", \"early_rust_1\", \"healthy_leaf_1\", \"late_leaf_spot_1\", \"nutrition_deficiency_1\", \"rust_1\"]  # Replace with your actual class names\n",
    "\n",
    "# Map numerical predictions to class names\n",
    "y_pred_mapped = [class_names[prediction] for prediction in y_pred]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_mapped)\n",
    "\n",
    "# Create a confusion matrix using the actual class names\n",
    "cm = confusion_matrix(y_true, y_pred_mapped, labels=class_names)\n",
    "\n",
    "# Generate a classification report using the actual class names\n",
    "report = classification_report(y_true, y_pred_mapped, target_names=class_names)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ce2f21-cfce-4986-9c42-daeda6f5b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_file)\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(image_path)\n",
    "                # Convert image to numpy array or do any preprocessing\n",
    "                # image = np.array(image)\n",
    "                \n",
    "                # Append image to the list\n",
    "                images.append(image)\n",
    "                \n",
    "                # Append label to the list\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {image_path}. Error: {e}\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df13527d-e842-4010-a8f8-6a3520c52122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the trained model\n",
    "def load_trained_model(model_path):\n",
    "    model = load_model(model_path)  # Load the model from .h5 file\n",
    "    return model\n",
    "\n",
    "# Assuming you have a dataset directory named 'test_dataset' containing subdirectories for each class\n",
    "dataset_dir = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/pretrained_testf'\n",
    "\n",
    "# Load test data (features) and corresponding true labels\n",
    "test_images, y_true = load_data(dataset_dir)\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/Inceptionv3.h5'  # Path to your saved model\n",
    "model = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44dbca9c-b584-4c86-9946-cf61ced0e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 271s 2s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "def predict_labels(model, images):\n",
    "    # Preprocess images according to DenseNet requirements\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # Resize image to target input dimensions expected by DenseNet (e.g., 224x224 for DenseNet121)\n",
    "        img = img.resize((256, 256))  # Adjust size as per your DenseNet model\n",
    "        \n",
    "        # Convert image to array and apply preprocessing required by DenseNet\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        processed_images.append(img_array)\n",
    "\n",
    "    # Convert processed images to numpy array\n",
    "    processed_images = np.array(processed_images)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(processed_images)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Assuming you have a pre-trained DenseNet model stored in the variable 'model'\n",
    "# And 'test_images' contains the images you want to make predictions on\n",
    "# 'test_images' is assumed to be a list of PIL Image objects\n",
    "\n",
    "# Make predictions using your trained model\n",
    "y_pred = predict_labels(model, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b310434a-bef8-4581-b1c3-7a134450a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "\n",
      "Confusion Matrix:\n",
      "[[312   0  71  13  13   0]\n",
      " [  1 305   0  22   0  81]\n",
      " [  1   0 360   0  48   0]\n",
      " [  5   0   0 400   0   0]\n",
      " [  1  51  24   3 330   1]\n",
      " [  1  26   0 106   6 270]]\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     early_leaf_spot_1       0.97      0.76      0.85       409\n",
      "          early_rust_1       0.80      0.75      0.77       409\n",
      "        healthy_leaf_1       0.79      0.88      0.83       409\n",
      "      late_leaf_spot_1       0.74      0.99      0.84       405\n",
      "nutrition_deficiency_1       0.83      0.80      0.82       410\n",
      "                rust_1       0.77      0.66      0.71       409\n",
      "\n",
      "              accuracy                           0.81      2451\n",
      "             macro avg       0.82      0.81      0.80      2451\n",
      "          weighted avg       0.82      0.81      0.80      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"early_leaf_spot_1\", \"early_rust_1\", \"healthy_leaf_1\", \"late_leaf_spot_1\", \"nutrition_deficiency_1\", \"rust_1\"]  # Replace with your actual class names\n",
    "\n",
    "# Map numerical predictions to class names\n",
    "y_pred_mapped = [class_names[prediction] for prediction in y_pred]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_mapped)\n",
    "\n",
    "# Create a confusion matrix using the actual class names\n",
    "cm = confusion_matrix(y_true, y_pred_mapped, labels=class_names)\n",
    "\n",
    "# Generate a classification report using the actual class names\n",
    "report = classification_report(y_true, y_pred_mapped, target_names=class_names)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f9ebff-4932-41ba-b000-be5162d33057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for image_file in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_file)\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(image_path)\n",
    "                # Convert image to numpy array or do any preprocessing\n",
    "                # image = np.array(image)\n",
    "                \n",
    "                # Append image to the list\n",
    "                images.append(image)\n",
    "                \n",
    "                # Append label to the list\n",
    "                labels.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {image_path}. Error: {e}\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd27839-d929-4960-a9f4-d21a3a60423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the trained model\n",
    "def load_trained_model(model_path):\n",
    "    model = load_model(model_path)  # Load the model from .h5 file\n",
    "    return model\n",
    "\n",
    "# Assuming you have a dataset directory named 'test_dataset' containing subdirectories for each class\n",
    "dataset_dir = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/pretrained_testf'\n",
    "\n",
    "# Load test data (features) and corresponding true labels\n",
    "test_images, y_true = load_data(dataset_dir)\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/Xception.h5'  # Path to your saved model\n",
    "model = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d15629d6-f7e0-4128-8494-99dd48503587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 383s 4s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "def predict_labels(model, images):\n",
    "    # Preprocess images according to DenseNet requirements\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # Resize image to target input dimensions expected by DenseNet (e.g., 224x224 for DenseNet121)\n",
    "        img = img.resize((256, 256))  # Adjust size as per your DenseNet model\n",
    "        \n",
    "        # Convert image to array and apply preprocessing required by DenseNet\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        processed_images.append(img_array)\n",
    "\n",
    "    # Convert processed images to numpy array\n",
    "    processed_images = np.array(processed_images)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(processed_images)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Assuming you have a pre-trained DenseNet model stored in the variable 'model'\n",
    "# And 'test_images' contains the images you want to make predictions on\n",
    "# 'test_images' is assumed to be a list of PIL Image objects\n",
    "\n",
    "# Make predictions using your trained model\n",
    "y_pred = predict_labels(model, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "251fe313-fffe-4d64-ba46-15f7fea3c44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n",
      "\n",
      "Confusion Matrix:\n",
      "[[285   0 106  12   6   0]\n",
      " [  0 405   2   2   0   0]\n",
      " [  0   0 407   0   2   0]\n",
      " [ 39   0   0 366   0   0]\n",
      " [  0   2  19   0 389   0]\n",
      " [  0 142   0   4   2 261]]\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "     early_leaf_spot_1       0.88      0.70      0.78       409\n",
      "          early_rust_1       0.74      0.99      0.85       409\n",
      "        healthy_leaf_1       0.76      1.00      0.86       409\n",
      "      late_leaf_spot_1       0.95      0.90      0.93       405\n",
      "nutrition_deficiency_1       0.97      0.95      0.96       410\n",
      "                rust_1       1.00      0.64      0.78       409\n",
      "\n",
      "              accuracy                           0.86      2451\n",
      "             macro avg       0.88      0.86      0.86      2451\n",
      "          weighted avg       0.88      0.86      0.86      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"early_leaf_spot_1\", \"early_rust_1\", \"healthy_leaf_1\", \"late_leaf_spot_1\", \"nutrition_deficiency_1\", \"rust_1\"]  # Replace with your actual class names\n",
    "\n",
    "# Map numerical predictions to class names\n",
    "y_pred_mapped = [class_names[prediction] for prediction in y_pred]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred_mapped)\n",
    "\n",
    "# Create a confusion matrix using the actual class names\n",
    "cm = confusion_matrix(y_true, y_pred_mapped, labels=class_names)\n",
    "\n",
    "# Generate a classification report using the actual class names\n",
    "report = classification_report(y_true, y_pred_mapped, target_names=class_names)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f556103-8f12-4399-b2c9-2d8ad849b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the trained model\n",
    "def load_trained_model(model_path):\n",
    "    model = load_model(model_path)  # Load the model from .h5 file\n",
    "    return model\n",
    "\n",
    "# Assuming you have a dataset directory named 'test_dataset' containing subdirectories for each class\n",
    "dataset_dir = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/pretrained_testf'\n",
    "\n",
    "# Load test data (features) and corresponding true labels\n",
    "test_images, y_true = load_data(dataset_dir)\n",
    "\n",
    "# Load your trained model\n",
    "model_path = r'Documents/Groundnut_Leaf_dataset/Groundnut_Leaf_dataset/Xception.h5'  # Path to your saved model\n",
    "model = load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb1f3320-b39a-4ff9-af8a-0be47adc73b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.80 GiB for an array with shape (2451, 256, 256, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 33\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_labels\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Assuming you have a pre-trained DenseNet model stored in the variable 'model'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# And 'test_images' contains the images you want to make predictions on\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 'test_images' is assumed to be a list of PIL Image objects\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Make predictions using your trained model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36mpredict_labels\u001b[1;34m(model, images)\u001b[0m\n\u001b[0;32m     15\u001b[0m     processed_images\u001b[38;5;241m.\u001b[39mappend(img_array)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert processed images to numpy array\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m processed_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(processed_images)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Make predictions using the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(processed_images)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.80 GiB for an array with shape (2451, 256, 256, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "def predict_labels(model, images):\n",
    "    # Preprocess images according to DenseNet requirements\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # Resize image to target input dimensions expected by DenseNet (e.g., 224x224 for DenseNet121)\n",
    "        img = img.resize((256, 256))  # Adjust size as per your DenseNet model\n",
    "        \n",
    "        # Convert image to array and apply preprocessing required by DenseNet\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        \n",
    "        processed_images.append(img_array)\n",
    "\n",
    "    # Convert processed images to numpy array\n",
    "    processed_images = np.array(processed_images)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(processed_images)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "# Assuming you have a pre-trained DenseNet model stored in the variable 'model'\n",
    "# And 'test_images' contains the images you want to make predictions on\n",
    "# 'test_images' is assumed to be a list of PIL Image objects\n",
    "\n",
    "# Make predictions using your trained model\n",
    "y_pred = predict_labels(model, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbe296-1ef4-4454-b595-605ebbd266c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
